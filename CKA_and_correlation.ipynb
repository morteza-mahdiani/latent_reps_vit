{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS4ZlAYTx9vZ",
        "outputId": "dd363930-283b-4be1-ef66-eabdee6f91b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surgeon-pytorch\n",
            "  Downloading surgeon_pytorch-0.0.4-py3-none-any.whl.metadata (649 bytes)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from surgeon-pytorch) (2.5.1+cu121)\n",
            "Collecting data-science-types>=0.2 (from surgeon-pytorch)\n",
            "  Downloading data_science_types-0.2.23-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->surgeon-pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->surgeon-pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->surgeon-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->surgeon-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->surgeon-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->surgeon-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6->surgeon-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->surgeon-pytorch) (3.0.2)\n",
            "Downloading surgeon_pytorch-0.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading data_science_types-0.2.23-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: data-science-types, surgeon-pytorch\n",
            "Successfully installed data-science-types-0.2.23 surgeon-pytorch-0.0.4\n"
          ]
        }
      ],
      "source": [
        "! pip install surgeon-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "#from surgeon_pytorch import Inspect, get_layers\n",
        "import os, random, pathlib, warnings, itertools, math\n",
        "from torchvision.transforms import Resize, ToTensor\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from PIL import Image\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from transformers import AutoModel, AutoFeatureExtractor\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "5d4r0EK0x-6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data"
      ],
      "metadata": {
        "id": "W7bsGbR6yDzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"khalidboussaroual/2d-geometric-shapes-17-shapes\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()  # This will prompt you to upload the `kaggle.json` file"
      ],
      "metadata": {
        "id": "ao3DzGsSyDXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Kaggle authentication\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d khalidboussaroual/2d-geometric-shapes-17-shapes -p /content/data --unzip\n"
      ],
      "metadata": {
        "id": "8Jqk7ORsyLom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/data/2D_Geometric_Shapes_Dataset'\n",
        "categories = os.listdir(data_dir)\n",
        "categories"
      ],
      "metadata": {
        "id": "VRFg56LYyR0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Function"
      ],
      "metadata": {
        "id": "rJV_FSMrya7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count classes and their samples\n",
        "def count_classes_and_samples(data_path):\n",
        "    class_counts = {}\n",
        "    class_names = []\n",
        "\n",
        "    for label in os.listdir(data_path):\n",
        "        label_path = os.path.join(data_path, label)\n",
        "        if os.path.isdir(label_path):\n",
        "            num_samples = len(os.listdir(label_path))\n",
        "            class_counts[label] = num_samples\n",
        "            class_names.append(label)\n",
        "\n",
        "    return class_names, class_counts\n",
        "\n",
        "def plot_class_samples(data_path, num_samples=3):\n",
        "    #class_names, class_counts = count_classes_and_samples(data_path)\n",
        "    class_names = ['square', 'circle', 'triangle', 'star', 'trapezoid']\n",
        "    class_counts = 5\n",
        "\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "    print(f\"Class names and sample counts: {class_counts}\")\n",
        "\n",
        "    # Adjust figure size for smaller images\n",
        "    plt.figure(figsize=(6, 6))\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(data_path, class_name)\n",
        "        images = os.listdir(class_path)[:num_samples]\n",
        "\n",
        "        for i, image_name in enumerate(images):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            try:\n",
        "                image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "                # Subplots with smaller images\n",
        "                plt.subplot(len(class_names), num_samples, idx * num_samples + i + 1)\n",
        "                plt.imshow(image)\n",
        "                plt.xticks([])\n",
        "                plt.yticks([])\n",
        "                if i == 1:  # Center the class name over the middle image\n",
        "                    plt.title(class_name, fontsize=8)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {image_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = \"dataset.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# define CKA\n",
        "def center_gram(gram):\n",
        "    \"\"\"Center a gram matrix.\"\"\"\n",
        "    if isinstance(gram, torch.Tensor):\n",
        "        gram = gram.numpy()  # Convert to NumPy\n",
        "\n",
        "    n = gram.shape[0]\n",
        "    unit = np.ones((n, n))\n",
        "    identity = np.eye(n)\n",
        "    return gram - unit @ gram / n - gram @ unit / n + unit @ gram @ unit / (n * n)\n",
        "\n",
        "def linear_CKA(X, Y):\n",
        "    \"\"\"Compute linear CKA similarity between two matrices X and Y.\"\"\"\n",
        "    # Center the gram matrices\n",
        "    X_centered = center_gram(X @ X.T)\n",
        "    Y_centered = center_gram(Y @ Y.T)\n",
        "\n",
        "    # Compute the CKA similarity\n",
        "    numerator = np.trace(X_centered @ Y_centered)\n",
        "    denominator = np.sqrt(np.trace(X_centered @ X_centered) * np.trace(Y_centered @ Y_centered))\n",
        "    return numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "\n",
        "# Define a function to process a batch\n",
        "def collate_fn(batch):\n",
        "    images, labels = zip(*batch)\n",
        "    # get image name\n",
        "    image_names = [os.path.basename(img_path) for img_path in images]\n",
        "    # Preprocess images using ViTImageProcessor\n",
        "    pixel_values = processor(images=[Image.open(img).convert(\"RGB\") for img in images], return_tensors=\"pt\")\n",
        "\n",
        "    #images = [processor(image, return_tensors=\"pt\")[\"pixel_values\"] for image in images]\n",
        "    # Concatenate into a single tensor\n",
        "    #images = torch.cat(images, dim=0)\n",
        "\n",
        "    #return images, torch.tensor(labels), image_names\n",
        "    return pixel_values[\"pixel_values\"], torch.tensor(labels)\n",
        "\n",
        "\n",
        "# Extract Representations from Selected Layers\n",
        "def extract_representations(model, layers, dataloader):\n",
        "    layer_outputs = {layer: [] for layer in layers}\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        layer_outputs[module.name].append(output.detach())\n",
        "\n",
        "    hooks = []\n",
        "    for name, module in model.named_modules():\n",
        "        if name in layers:\n",
        "            module.name = name\n",
        "            hooks.append(module.register_forward_hook(hook_fn))\n",
        "\n",
        "    # Pass data through the model\n",
        "    for batch in dataloader:\n",
        "\n",
        "        images, labels = batch\n",
        "        with torch.no_grad():\n",
        "\n",
        "            _ = model(images)\n",
        "\n",
        "    # Aggregate results and remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    return {layer: torch.cat(layer_outputs[layer]) for layer in layers}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oKg85UP4ycKv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "NJrY2hulyno6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Prepare dataset\n",
        "result = ['square', 'circle', 'triangle', 'star', 'trapezoid']\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for class_idx, category in enumerate(result):\n",
        "    category_path = os.path.join(data_dir, category)\n",
        "    if os.path.isdir(category_path):\n",
        "\n",
        "        selected_images = [\n",
        "            os.path.join(category_path, img)\n",
        "            for img in os.listdir(category_path)\n",
        "            if img.endswith((\".png\", \".jpg\", \".jpeg\")) and\n",
        "               any(img.endswith(f\"_{i}.{ext}\") for i in range(1, 16) for ext in [\"png\", \"jpg\", \"jpeg\"])\n",
        "        ]\n",
        "\n",
        "        #selected_images = image_files[:num_images_per_class]\n",
        "        data.extend(selected_images)\n",
        "        labels.extend([class_idx] * len(selected_images))\n",
        "\n",
        "\n",
        "# Wrap data into a PyTorch Dataset\n",
        "dataset = list(zip(data, labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "HPpp9qmlylOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the sample data"
      ],
      "metadata": {
        "id": "1IrZkVuvy6tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the plotting function\n",
        "plot_class_samples(data_dir, num_samples=5)"
      ],
      "metadata": {
        "id": "KWYRjfnpy3FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CKA analysis in model"
      ],
      "metadata": {
        "id": "QGAQiMTzy5_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [\"google/vit-large-patch16-224\",\n",
        "              \"facebook/vit-mae-large\", \"microsoft/beit-large-patch16-512\"]\n",
        "\n",
        "cka_similarities = []\n",
        "\n",
        "for n in range(3):\n",
        "# step 1: Initialize ViT Image Processor\n",
        "  model_name = model_names[n]  # supervised learning\n",
        "  processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# step 2: Create DataLoader\n",
        "  dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Step 3: Load the Vision Transformer model\n",
        "  model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "  model.eval()\n",
        "\n",
        "# step 4. Define layers to analyze\n",
        "  layers_to_analyze = [f'encoder.layer.{i}.output.dense' for i in range(1, 24)]\n",
        "  #layers_to_analyze = [f'encoder.layer.{i}.mlp.fc2' for i in range(1, 24)]\n",
        "\n",
        "# step 5. Extract Representations from Selected Layers\n",
        "  representations = extract_representations(model, layers_to_analyze, dataloader)\n",
        "\n",
        "# Step 6: Compute CKA for Layer Representations\n",
        "  layer_keys = list(representations.keys())\n",
        "  n_layers = len(layer_keys)\n",
        "  cka_similarity = torch.zeros((n_layers, n_layers))\n",
        "\n",
        "  for i in range(n_layers):\n",
        "    for j in range(n_layers):\n",
        "        X = representations[layer_keys[i]].view(representations[layer_keys[i]].shape[0], -1)\n",
        "        Y = representations[layer_keys[j]].view(representations[layer_keys[j]].shape[0], -1)\n",
        "        cka = linear_CKA(X, Y)\n",
        "        cka_similarity[i, j] = cka\n",
        "\n",
        "  cka_similarities.append(cka_similarity)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wes08HVBy_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for model facebook/dinov2-large-imagenet1k-1-layer\n"
      ],
      "metadata": {
        "id": "sRb65IuwzJhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Define transformations\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),  # Resize to model input size\n",
        "    ToTensor(),          # Convert image to tensor\n",
        "])\n",
        "\n",
        "# Load the dataset using ImageFolder\n",
        "dataset_fordino = ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "# Get class-to-index mapping\n",
        "class_to_idx = dataset_fordino.class_to_idx\n",
        "\n",
        "# Filter dataset for specific classes\n",
        "filtered_indices = []\n",
        "for target_class in result:\n",
        "    class_index = class_to_idx[target_class]\n",
        "    # Get all indices for the selected class\n",
        "    class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_index]\n",
        "    # Take only the first 10 images for this class\n",
        "    filtered_indices.extend(class_indices[:10])\n",
        "\n",
        "# Create a subset of the dataset\n",
        "filtered_dataset = Subset(dataset_fordino, filtered_indices)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader_fordino = DataLoader(filtered_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "model_name = \"facebook/dinov2-large-imagenet1k-1-layer\"  # supervised learning\n",
        "\n",
        "# Step 3: Load the Vision Transformer model\n",
        "model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "# step 4. Define layers to analyze\n",
        "layers_to_analyze = [f'encoder.layer.{i}.mlp.fc2' for i in range(1, 24)]\n",
        "# step 5. Extract Representations from Selected Layers\n",
        "representations = extract_representations(model, layers_to_analyze, dataloader_fordino)\n",
        "\n",
        "# Step 6: Compute CKA for Layer Representations\n",
        "layer_keys = list(representations.keys())\n",
        "n_layers = len(layer_keys)\n",
        "cka_similarity = torch.zeros((n_layers, n_layers))\n",
        "\n",
        "for i in range(n_layers):\n",
        "   for j in range(n_layers):\n",
        "        X = representations[layer_keys[i]].view(representations[layer_keys[i]].shape[0], -1)\n",
        "        Y = representations[layer_keys[j]].view(representations[layer_keys[j]].shape[0], -1)\n",
        "        cka = linear_CKA(X, Y)\n",
        "        cka_similarity[i, j] = cka\n",
        "\n",
        "cka_similarities.append(cka_similarity)\n"
      ],
      "metadata": {
        "id": "9dJ5VXtHzIXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"vit_large_patch16_224.augreg_in21k\"\n"
      ],
      "metadata": {
        "id": "1objLZVhzMyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "id": "f7W8FMECzPu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize, ToTensor, Compose\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = \"vit_large_patch16_224.augreg_in21k\"\n",
        "model = timm.create_model(model_name, pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define the dataset\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),  # Resize to match model input size\n",
        "    ToTensor(),  # Convert image to PyTorch tensor\n",
        "])\n",
        "# Load the dataset using ImageFolder\n",
        "dataset_fortimm= ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "# Create a subset of the dataset\n",
        "filtered_dataset = Subset(dataset_fortimm, filtered_indices)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader_fortimm = DataLoader(filtered_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Example usage\n",
        "#for batch in dataloader_fortimm:\n",
        "#    images, labels = batch\n",
        "#    print(\"Batch image tensor shape:\", images.shape)  # Should be [batch_size, 3, 224, 224]\n",
        "#    print(\"Batch labels:\", labels)\n",
        "#    break\n",
        "\n",
        "# Define the layers to analyze\n",
        "layers_to_analyze = [f'blocks.{i}.mlp.fc2' for i in range(1, 24)]\n",
        "\n",
        "representations = extract_representations(model, layers_to_analyze, dataloader_fortimm)\n",
        "\n",
        "# Step 6: Compute CKA for Layer Representations\n",
        "layer_keys = list(representations.keys())\n",
        "n_layers = len(layer_keys)\n",
        "cka_similarity = torch.zeros((n_layers, n_layers))\n",
        "\n",
        "for i in range(n_layers):\n",
        "   for j in range(n_layers):\n",
        "        X = representations[layer_keys[i]].view(representations[layer_keys[i]].shape[0], -1)\n",
        "        Y = representations[layer_keys[j]].view(representations[layer_keys[j]].shape[0], -1)\n",
        "        cka = linear_CKA(X, Y)\n",
        "        cka_similarity[i, j] = cka\n",
        "\n",
        "cka_similarities.append(cka_similarity)\n"
      ],
      "metadata": {
        "id": "duo6VACczZqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "QnKTyNClzf0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "model_names = [\"vit\",\"mae\", \"beit\", 'dinov2', \"vit_augreg\"]\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))  # 2x2 grid of subplots\n",
        "\n",
        "# Flatten axes for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(5):\n",
        "    ax = axes[i]\n",
        "    sns.heatmap(cka_similarities[i], cmap='coolwarm', ax=ax, cbar=True,\n",
        "                cbar_kws={'orientation': 'horizontal', 'pad': 0.1})\n",
        "    ax.set_title(model_names[i])\n",
        "\n",
        "# Turn off the last axis (the empty one)\n",
        "axes[-1].axis('off')\n",
        "\n",
        "# Adjust layout to make space for the color bar\n",
        "plt.subplots_adjust(bottom=0.2)  # Increase bottom space to ensure room for color bar\n",
        "\n",
        "fig.suptitle('Comparison of CKA Similarities Across Models', fontsize=16)\n",
        "\n",
        "# Display the plot\n",
        "# Display the plot\n",
        "save_path = \"_plots.png\"\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_path, dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_LDFFKYPzfs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute the correlation between the 5 heatmaps to quantify their similarity."
      ],
      "metadata": {
        "id": "8dK9nLwqzkry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_correlation(matrix1, matrix2):\n",
        "    vec1 = matrix1.flatten()\n",
        "    vec2 = matrix2.flatten()\n",
        "    return np.corrcoef(vec1, vec2)[0, 1]\n",
        "\n",
        "def compute_pearson(matrix1, matrix2):\n",
        "    vec1 = matrix1.flatten()\n",
        "    vec2 = matrix2.flatten()\n",
        "    spearman_corr, _ = spearmanr(vec1, vec2)\n",
        "    return spearman_corr\n",
        "\n",
        "def compute_cosine(matrix1, matrix2):\n",
        "    vec1 = matrix1.flatten()\n",
        "    vec2 = matrix2.flatten()\n",
        "    cosine_sim = cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0, 0]\n",
        "    return cosine_sim\n",
        "\n",
        "# Store pairwise results\n",
        "corr_matrix = np.zeros((len(cka_similarities), len(cka_similarities)))\n",
        "pear_matrix = np.zeros((len(cka_similarities), len(cka_similarities)))\n",
        "cosine_matrix = np.zeros((len(cka_similarities), len(cka_similarities)))\n",
        "\n",
        "# Fill the matrix with pairwise correlations\n",
        "for i, j in product(range(len(cka_similarities)), repeat=2):\n",
        "    corr_matrix[i, j] = compute_correlation(cka_similarities[i], cka_similarities[j])\n",
        "    pear_matrix[i, j] = compute_pearson(cka_similarities[i], cka_similarities[j])\n",
        "    cosine_matrix[i, j] = compute_cosine(cka_similarities[i], cka_similarities[j])\n",
        "\n",
        "# Convert to a DataFrame for readability\n",
        "corr_df = pd.DataFrame(corr_matrix, columns=model_names,\n",
        "                       index=model_names)\n",
        "\n",
        "pear_df = pd.DataFrame(pear_matrix, columns=model_names,\n",
        "                       index=model_names)\n",
        "\n",
        "cosine_df = pd.DataFrame(cosine_matrix, columns=model_names,\n",
        "                       index=model_names)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MZDAD1iBzhye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with 1 row and 3 columns\n",
        "fig, axs = plt.subplots(1, 3, figsize=(30, 10))  # 1 row, 3 columns\n",
        "\n",
        "# Plot the first heatmap\n",
        "sns.heatmap(corr_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", ax=axs[0])\n",
        "axs[0].set_title(\"Pairwise Correlation Between models\")\n",
        "\n",
        "# Plot the second heatmap\n",
        "sns.heatmap(pear_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", ax=axs[1])\n",
        "axs[1].set_title(\"Pairwise Pearson Correlation Between models\")\n",
        "\n",
        "# Plot the third heatmap\n",
        "sns.heatmap(cosine_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", ax=axs[2])\n",
        "axs[2].set_title(\"Pairwise cosine similarity Between models\")\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cTMGx7uFztCx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}